---

# ------------------------------------------------------------------------------
# General Properties
# ------------------------------------------------------------------------------
id: "s3_csv_to_pg"                     # Unique identifier of the tap
name: "Sample CSV files on S3"         # Name of the tap
type: "tap-s3-csv"                     # !! THIS SHOULD NOT CHANGE !!
owner: "somebody@foo.com"              # Data owner to contact


# ------------------------------------------------------------------------------
# Source (Tap) - S3 connection details
# ------------------------------------------------------------------------------
db_conn:
  aws_access_key_id: "${TAP_S3_CSV_AWS_KEY}"                    # Plain string or vault encrypted
  aws_secret_access_key: "${TAP_S3_CSV_AWS_SECRET_ACCESS_KEY}"  # Plain string or vault encrypted
  bucket: "${TAP_S3_CSV_BUCKET}"                                # S3 Bucket name
  start_date: "2000-01-01"                                      # File before this data will be excluded

 
# ------------------------------------------------------------------------------
# Destination (Target) - Target properties
# Connection details should be in the relevant target YAML file
# ------------------------------------------------------------------------------
target: "postgres_dwh"                    # ID of the target connector where the data will be loaded
batch_size_rows: 20000                    # Batch size for the stream to optimise load performance
default_target_schema: "s3_feeds"         # Target schema where the data will be loaded
default_target_schema_select_permissions:  # Optional: Grant SELECT on schema and tables that created
  - grp_power
primary_key_required: False             # Optional: in case you want to load tables without key
                                          #            properties, uncomment this. Please note
                                          #            that files without primary keys will not
                                          #            be de-duplicated and could cause
                                          #            duplicates. Aloways try selecting
                                          #            a reasonable key from the CSV file


# ------------------------------------------------------------------------------
# Source to target Schema mapping
# ------------------------------------------------------------------------------
schemas:
  - source_schema: "s3_feeds" # This is mandatory, but can be anything in this tap type
    target_schema: "s3_feeds" # Target schema in the destination Data Warehouse
    
    # List of CSV files to destination tables
    tables:

      # Every file in S3 bucket that matches the search pattern will be loaded into this table
      - table_name: "people"
        replication_method: "FULL_TABLE"
        s3_csv_mapping:
          search_pattern: "^greenhouse/mock_data_1.csv$"
          date_overrides:
            - birth_date
        transformations:
          - column: "email"
            type: "HASH"
          - column: "group"
            type: "HASH"

      - table_name: "countries"
        s3_csv_mapping:
          search_pattern: "^greenhouse/mock_data_2.csv$"
          key_properties: ["id"]
